from typing import List, Tuple
import torch
import torch.nn as nn
import torchvision.models as models
from torchvision.transforms import v2

from .model.mine import MINE

class VGGPerceptualLoss(nn.Module):
    """
    Calculates perceptual loss using a pre-trained VGG19 network.
    This loss is composed of a content loss and a style loss.
    """
    def __init__(self, device: torch.device):
        super(VGGPerceptualLoss, self).__init__()
        vgg = models.vgg19(weights='DEFAULT').features[:29].to(device).eval()
        for param in vgg.parameters():
            param.requires_grad = False
        self.vgg = vgg
        
        self.indexes = [0, 5, 10, 19, 28]
        self.normalize = v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        self.criterion = nn.L1Loss()

    def _gram_matrix(self, x: torch.Tensor) -> torch.Tensor:
        """Calculates the Gram matrix for a given tensor."""
        a, b, c, d = x.size()
        features = x.view(a * b, c * d)
        G = torch.mm(features, features.t())
        return G.div(a * b * c * d)

    def forward(self, generated: torch.Tensor, real: torch.Tensor, degraded_style: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Calculates the content and style losses.
        Args:
            generated (torch.Tensor): The image generated by the VAE.
            real (torch.Tensor): The original high-resolution image.
            degraded_style (torch.Tensor): The low-resolution image used for style.
        Returns:
            A tuple containing (content_loss, style_loss).
        """
        generated_vgg = self._get_features(generated)
        real_vgg = self._get_features(real)
        degraded_style_vgg = self._get_features(degraded_style)

        # Content Loss: Compare features of generated and real images
        content_loss = self.criterion(generated_vgg[1], real_vgg[1])

        # Style Loss: Compare Gram matrices of generated and style images
        style_loss = 0.0
        for gen_feat, style_feat in zip(generated_vgg, degraded_style_vgg):
            gram_gen = self._gram_matrix(gen_feat)
            gram_style = self._gram_matrix(style_feat)
            style_loss += self.criterion(gram_gen, gram_style)
        
        return content_loss, style_loss

    def _get_features(self, x: torch.Tensor) -> List[torch.Tensor]:
        """Extracts VGG features from an image."""
        x = self.normalize(x)
        features = []
        for index, layer in enumerate(self.vgg):
            x = layer(x)
            if index in self.indexes:
                features.append(x)
        return features

def mutual_information_loss(mine_net: MINE, img_degraded: torch.Tensor, img_reconstructed: torch.Tensor) -> torch.Tensor:
    """
    Calculates the mutual information loss using the MINE estimate.
    We want to maximize MI, so the loss for the VAE is -MI.
    """
    marginal = img_reconstructed[torch.randperm(img_degraded.shape[0])]
    t = torch.mean(mine_net(img_degraded, img_reconstructed))
    et = torch.mean(torch.exp(mine_net(img_degraded, marginal) - 1))
    return t - et

def kl_divergence_loss(mean: torch.Tensor, log_var: torch.Tensor) -> torch.Tensor:
    """
    Calculates the KL divergence loss for the VAE.
    """
    return -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())